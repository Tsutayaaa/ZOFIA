import os
import sys
import cv2
import pandas as pd
import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple
import traceback
from datetime import datetime
from tqdm import tqdm
import numpy as np

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from fishmouth_behavior.preprocess import video_clip
    from fishmouth_behavior.mouth_detect import detector
    from fishmouth_behavior.analysis.analyzer import FishMouthAnalyzer
    from fishmouth_behavior.visualization import plotter
    from fishmouth_behavior.visualization import preview
except ImportError as e:
    print(f"FATAL: A core module could not be imported: {e}", flush=True)
    print("Please ensure the 'src' directory is in your PYTHONPATH or structured correctly.", flush=True)
    sys.exit(1)

# è‡ªå®šä¹‰JSONè½¬æ¢å™¨ï¼Œç”¨äºå¤„ç†Numpyæ•°æ®ç±»å‹
class NumpyEncoder(json.JSONEncoder):
    """ 
    ä¸€ä¸ªç‰¹æ®Šçš„JSONç¼–ç å™¨ï¼Œå¯ä»¥å°†Numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œ
    ä»è€Œè§£å†³ 'Object of type int64 is not JSON serializable' é”™è¯¯ã€‚
    """
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)


class HierarchicalAnalysisPipeline:
    """
    å®Œæ•´çš„é±¼å˜´è¡Œä¸ºåˆ†å±‚æ—¶é—´åˆ†æç®¡é“ã€‚
    éµå¾ªâ€œæ ‡å‡†åŒ– -> é¢„å¤„ç† -> åˆ†å±‚åˆ‡ç‰‡ -> ç‹¬ç«‹åˆ†æ -> æ±‡æ€»æŠ¥å‘Šâ€çš„æµç¨‹ã€‚
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.batch_summary = []

    def _standardize_video_duration(self, input_path: str, output_path: str, target_duration_sec: int = 300) -> bool:
        """
        å°†è¾“å…¥è§†é¢‘æ ‡å‡†åŒ–ä¸ºå›ºå®šçš„ç›®æ ‡æ—¶é•¿ã€‚
        """
        try:
            cap = cv2.VideoCapture(input_path)
            if not cap.isOpened():
                print(f"  é”™è¯¯ï¼šæ— æ³•æ‰“å¼€è§†é¢‘ {input_path}", flush=True)
                return False
                
            fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            
            writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))
            
            target_frames = int(target_duration_sec * fps)
            
            with tqdm(total=target_frames, desc=f"  æ ‡å‡†åŒ–å¸§è¿›åº¦", unit="frame", leave=False) as pbar:
                frames_written = 0
                while frames_written < target_frames:
                    ret, frame = cap.read()
                    if not ret:
                        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        ret, frame = cap.read()
                        if not ret:
                            print("  é”™è¯¯ï¼šæ— æ³•ä»è§†é¢‘ä¸­è¯»å–å¸§ï¼Œå³ä½¿æ˜¯å¾ªç¯æ’­æ”¾ã€‚", flush=True)
                            break
                            
                    writer.write(frame)
                    frames_written += 1
                    pbar.update(1)
                
            cap.release()
            writer.release()
            
            print(f"  âœ“ å·²å°†è§†é¢‘æ ‡å‡†åŒ–ä¸º {target_duration_sec} ç§’ã€‚", flush=True)
            return True
            
        except Exception as e:
            print(f"  é”™è¯¯ï¼šè§†é¢‘æ ‡å‡†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}", flush=True)
            traceback.print_exc()
            return False

    def _create_time_segments(self, video_path: str, output_dir: str) -> Dict[str, str]:
        """
        å°†æ ‡å‡†åŒ–çš„è§†é¢‘åˆ†å‰²æˆå¤šä¸ªé¢„å®šä¹‰çš„æ—¶é—´æ®µã€‚
        """
        segments = {}
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"  é”™è¯¯ï¼šæ— æ³•æ‰“å¼€è§†é¢‘ {video_path} è¿›è¡Œåˆ‡ç‰‡ã€‚", flush=True)
            return segments
            
        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        
        time_ranges = [
            (0, 300, "full_5min"),
            (0, 60, "min_1"), (60, 120, "min_2"), (120, 180, "min_3"), (180, 240, "min_4"), (240, 300, "min_5"),
            (0, 10, "min1_seg1_10s"), (10, 20, "min1_seg2_10s"), (20, 30, "min1_seg3_10s"),
            (30, 40, "min1_seg4_10s"), (40, 50, "min1_seg5_10s"), (50, 60, "min1_seg6_10s"),
        ]
        
        with tqdm(total=len(time_ranges), desc="  åˆ›å»ºæ—¶é—´ç‰‡æ®µ", unit="segment", leave=False) as pbar:
            for start_sec, end_sec, segment_name in time_ranges:
                pbar.set_description(f"  åˆ›å»ºç‰‡æ®µ [{segment_name}]")
                output_path = os.path.join(output_dir, f"{segment_name}.mp4")
                segments[segment_name] = output_path
                
                cap.set(cv2.CAP_PROP_POS_FRAMES, int(start_sec * fps))
                
                frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))
                
                end_frame_pos = int(end_sec * fps)
                
                while cap.get(cv2.CAP_PROP_POS_FRAMES) < end_frame_pos:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    writer.write(frame)
                    
                writer.release()
                pbar.update(1)
            
        cap.release()
        print(f"  âœ“ å·²æˆåŠŸåˆ›å»º {len(time_ranges)} ä¸ªæ—¶é—´ç‰‡æ®µã€‚", flush=True)
        return segments

    def _analyze_segment(self, segment_name: str, segment_path: str, output_dirs: Dict) -> Dict:
        """å¯¹å•ä¸ªæ—¶é—´ç‰‡æ®µè¿›è¡Œå®Œæ•´çš„åˆ†æã€‚"""
        print(f"\n    --- å¼€å§‹åˆ†ææ—¶é—´æ®µ: {segment_name} ---", flush=True)
        
        print(f"      [1/3] å¼€å§‹ç‰¹å¾æ£€æµ‹...", flush=True)
        detection_df = detector.process_video_and_extract_data(
            video_source_path=segment_path,
            yolo_model_path_eyes=self.config['eye_model'],
            yolo_model_path_mouth=self.config['mouth_model'],
            mouth_conf_threshold=self.config.get('mouth_conf_threshold', 0.3),
            eye_conf_threshold=self.config.get('eye_conf_threshold', 0.3)
        )
        print(f"      [1/3] ç‰¹å¾æ£€æµ‹å®Œæˆã€‚", flush=True)
        
        if detection_df is None or detection_df.empty:
            print(f"      [è­¦å‘Š] {segment_name} çš„ç‰¹å¾æ£€æµ‹å¤±è´¥ï¼Œè·³è¿‡åˆ†æã€‚", flush=True)
            return {'status': 'Detection Failed'}
            
        detection_csv_path = Path(output_dirs['detection_data']) / f'{segment_name}_detection.csv'
        detection_df.to_csv(detection_csv_path, index=False)
        
        print(f"      [2/3] å¼€å§‹è¡Œä¸ºåˆ†æ...", flush=True)
        analyzer = FishMouthAnalyzer(detection_df)
        events = analyzer.analyze(**self.config.get('analysis_params', {}))
        print(f"      [2/3] è¡Œä¸ºåˆ†æå®Œæˆã€‚", flush=True)
        
        events_json_path = Path(output_dirs['analysis_results']) / f'{segment_name}_events.json'
        with open(events_json_path, 'w', encoding='utf-8') as f:
            json.dump(events, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
            
        print(f"      [3/3] å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...", flush=True)
        plot_path = Path(output_dirs['visualizations']) / f'{segment_name}_analysis_plot.png'
        plotter.plot_analysis_results(
            df_full=analyzer.df_processed,
            events=events,
            time_col='timestamp_sec',
            area_col='normalized_area',
            output_image_path=str(plot_path)
        )
        print(f"      [3/3] å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæ¯•ã€‚", flush=True)
        
        print(f"      âœ“ {segment_name} åˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(events)} ä¸ªäº‹ä»¶ã€‚", flush=True)
        return {
            'status': 'Success',
            'detection_data_path': str(detection_csv_path),
            'events_data_path': str(events_json_path),
            'plot_path': str(plot_path),
            'events': events,
            'event_count': len(events)
        }

    def process_single_video(self, video_path: Path) -> None:
        """å¤„ç†å•ä¸ªè§†é¢‘çš„å®Œæ•´åˆ†å±‚åˆ†ææµç¨‹ã€‚"""
        video_name = video_path.stem
        print(f"\n{'='*70}", flush=True)
        print(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name} [æ—¶é—´: {datetime.now()}]", flush=True)
        print(f"{'='*70}", flush=True)
        
        base_output_dir = Path(self.config['output_dir']) / video_name
        dirs = {
            'base': base_output_dir,
            'preprocessed': base_output_dir / '01_preprocessed',
            'segments': base_output_dir / '02_time_segments', 
            'detection_data': base_output_dir / '03_detection_data',
            'analysis_results': base_output_dir / '04_analysis_results',
            'visualizations': base_output_dir / '05_visualizations'
        }
        for dir_path in dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)
            
        try:
            print(f"\n[æ­¥éª¤ 1/5] å¼€å§‹è§†é¢‘æ—¶é•¿æ ‡å‡†åŒ–...", flush=True)
            standardized_path = dirs['preprocessed'] / 'standardized_5min.mp4'
            if not self._standardize_video_duration(str(video_path), str(standardized_path)):
                raise RuntimeError("è§†é¢‘æ ‡å‡†åŒ–å¤±è´¥ã€‚")
            print(f"[æ­¥éª¤ 1/5] è§†é¢‘æ—¶é•¿æ ‡å‡†åŒ–å®Œæˆã€‚", flush=True)
                
            print(f"\n[æ­¥éª¤ 2/5] å¼€å§‹è§†é¢‘é¢„å¤„ç†ï¼ˆé±¼å¤´è£å‰ªï¼‰...", flush=True)
            preprocessed_path = dirs['preprocessed'] / 'cropped_5min.mp4'
            video_clip.run_clipping(
                video_path=str(standardized_path),
                output_video_path=str(preprocessed_path),
                yolo_model_path=self.config['head_model'],
                confidence_thresh=self.config.get('head_conf_threshold', 0.5)
            )
            print(f"[æ­¥éª¤ 2/5] è§†é¢‘é¢„å¤„ç†å®Œæˆã€‚", flush=True)
            
            print(f"\n[æ­¥éª¤ 3/5] å¼€å§‹åˆ›å»ºæ—¶é—´åˆ†æ®µ...", flush=True)
            segments = self._create_time_segments(str(preprocessed_path), str(dirs['segments']))
            print(f"[æ­¥éª¤ 3/5] åˆ›å»ºæ—¶é—´åˆ†æ®µå®Œæˆã€‚", flush=True)
            
            print(f"\n[æ­¥éª¤ 4/5] å¼€å§‹å¯¹æ‰€æœ‰æ—¶é—´ç‰‡æ®µè¿›è¡Œç‹¬ç«‹åˆ†æ...", flush=True)
            segment_analysis_results = {}
            for name, path in tqdm(segments.items(), desc="  åˆ†ææ—¶é—´ç‰‡æ®µ", unit="segment"):
                if Path(path).exists():
                    result = self._analyze_segment(name, str(path), dirs)
                    segment_analysis_results[name] = result
                else:
                    print(f"  [è­¦å‘Š] æ—¶é—´ç‰‡æ®µæ–‡ä»¶ä¸å­˜åœ¨: {path}ï¼Œè·³è¿‡ã€‚", flush=True)
            print(f"[æ­¥éª¤ 4/5] æ‰€æœ‰æ—¶é—´ç‰‡æ®µåˆ†æå®Œæˆã€‚", flush=True)
            
            print(f"\n[æ­¥éª¤ 5/5] å¼€å§‹ç”Ÿæˆæ­¤è§†é¢‘çš„æ±‡æ€»æŠ¥å‘Š...", flush=True)
            video_summary = self._generate_video_summary(video_name, segment_analysis_results, dirs)
            self.batch_summary.append(video_summary)
            print(f"[æ­¥éª¤ 5/5] æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå®Œæˆã€‚", flush=True)
            
            print(f"\nâœ… è§†é¢‘ {video_name} å¤„ç†å®Œæˆ!", flush=True)

        except Exception as e:
            print(f"\nâŒ å¤„ç†è§†é¢‘ {video_name} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", flush=True)
            traceback.print_exc()

    def _generate_video_summary(self, video_name: str, segment_results: Dict, dirs: Dict) -> Dict:
        """ä¸ºå•ä¸ªè§†é¢‘ç”Ÿæˆè¯¦ç»†çš„æ±‡æ€»æ–‡ä»¶ã€‚"""
        summary_data_for_csv = []
        for segment_name, results in segment_results.items():
            if results.get('status') != 'Success':
                continue
            
            events = results.get('events', [])
            row = {'segment': segment_name, 'event_count': len(events)}
            
            if events:
                durations = [e['duration_s'] for e in events]
                row['avg_duration_s'] = round(sum(durations) / len(durations), 2)
                row['max_duration_s'] = round(max(durations), 2)
                row['min_duration_s'] = round(min(durations), 2)
            else:
                row.update({'avg_duration_s': 0, 'max_duration_s': 0, 'min_duration_s': 0})
            
            summary_data_for_csv.append(row)
        
        summary_df = pd.DataFrame(summary_data_for_csv)
        summary_csv_path = dirs['base'] / f'{video_name}_summary_report.csv'
        summary_df.to_csv(summary_csv_path, index=False)
        print(f"  âœ“ å·²ä¿å­˜åˆ†æ®µå¯¹æ¯”æŠ¥å‘Š: {summary_csv_path}", flush=True)
        
        report = {
            'video_name': video_name,
            'summary_csv_path': str(summary_csv_path),
            'segments': segment_results
        }
        report_path = dirs['base'] / f'{video_name}_processing_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            for seg_name, seg_data in report['segments'].items():
                seg_data.pop('events', None)
            json.dump(report, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        return report

    def run_batch_processing(self, video_files: List[Path]):
        """
        æ ¹æ®æä¾›çš„æ–‡ä»¶åˆ—è¡¨è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚
        """
        print(f"å‘ç° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶å¾…å¤„ç†ã€‚", flush=True)
        
        for video_path in tqdm(video_files, desc="è§†é¢‘æ‰¹å¤„ç†è¿›åº¦"):
            self.process_single_video(video_path)
        
        self.generate_final_batch_summary()

    def generate_final_batch_summary(self):
        """ä¸ºæ•´ä¸ªæ‰¹æ¬¡ç”Ÿæˆæœ€ç»ˆçš„æ±‡æ€»æŠ¥å‘Šã€‚"""
        if not self.batch_summary:
            print("è­¦å‘Š: æ²¡æœ‰å¯ä¾›ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šçš„æ•°æ®ã€‚", flush=True)
            return
            
        final_summary_path = Path(self.config['output_dir']) / 'batch_summary.json'
        
        for video_report in self.batch_summary:
            for seg_name, seg_data in video_report.get('segments', {}).items():
                seg_data.pop('events', None)

        batch_summary_data = {
            'run_datetime': datetime.now().isoformat(),
            'total_videos_processed': len(self.batch_summary),
            'processing_config': {k: v for k, v in self.config.items() if k != 'output_dir'},
            'video_reports': self.batch_summary
        }
        
        with open(final_summary_path, 'w', encoding='utf-8') as f:
            json.dump(batch_summary_data, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
            
        print(f"\n{'='*70}", flush=True)
        print(f"ğŸ‰ æ‰¹å¤„ç†å®Œæˆï¼æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜è‡³: {final_summary_path}", flush=True)
        print(f"{'='*70}", flush=True)


def main():
    parser = argparse.ArgumentParser(
        description='é±¼å˜´è¡Œä¸ºåˆ†å±‚æ—¶é—´åˆ†æå·¥ä½œæµ',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ¥å— --input_dir æˆ– --input_file ---
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--input_dir', help='åŒ…å«è¾“å…¥è§†é¢‘çš„ç›®å½•è·¯å¾„ã€‚')
    group.add_argument('--input_file', help='å•ä¸ªè¾“å…¥è§†é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚')
    
    parser.add_argument('--output_dir', required=True, help='ç”¨äºä¿å­˜æ‰€æœ‰ç»“æœçš„æ€»è¾“å‡ºç›®å½•è·¯å¾„ã€‚')
    parser.add_argument('--head_model', required=True, help='é±¼å¤´æ£€æµ‹æ¨¡å‹ (.pt) çš„è·¯å¾„ã€‚')
    parser.add_argument('--eye_model', required=True, help='çœ¼éƒ¨æ£€æµ‹æ¨¡å‹ (.pt) çš„è·¯å¾„ã€‚')
    parser.add_argument('--mouth_model', required=True, help='å˜´éƒ¨æ£€æµ‹æ¨¡å‹ (.pt) çš„è·¯å¾„ã€‚')

    args = parser.parse_args()

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ ¹æ®è¾“å…¥å‚æ•°æ„å»ºå¾…å¤„ç†æ–‡ä»¶åˆ—è¡¨ ---
    video_files_to_process = []
    if args.input_file:
        input_path = Path(args.input_file)
        if not input_path.is_file():
            print(f"âŒ è‡´å‘½é”™è¯¯: æŒ‡å®šçš„è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}", flush=True)
            sys.exit(1)
        video_files_to_process.append(input_path)
    elif args.input_dir:
        input_path = Path(args.input_dir)
        if not input_path.is_dir():
            print(f"âŒ è‡´å‘½é”™è¯¯: æŒ‡å®šçš„è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}", flush=True)
            sys.exit(1)
        video_files_to_process = sorted([p for p in input_path.rglob('*') if p.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv']])

    if not video_files_to_process:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„è§†é¢‘æ–‡ä»¶ã€‚", flush=True)
        sys.exit(1)

    # éªŒè¯æ¨¡å‹è·¯å¾„
    for p in [args.head_model, args.eye_model, args.mouth_model]:
        if not Path(p).exists():
            print(f"âŒ è‡´å‘½é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {p}", flush=True)
            sys.exit(1)

    # æ„å»ºé…ç½®å­—å…¸
    config = {
        'output_dir': args.output_dir,
        'head_model': args.head_model,
        'eye_model': args.eye_model,
        'mouth_model': args.mouth_model,
        'head_conf_threshold': 0.5,
        'mouth_conf_threshold': 0.3,
        'eye_conf_threshold': 0.01,
        'analysis_params': {
            'peak_prominence': 0.01,
            'peak_height_min': 0.01,
            'peak_distance_samples': 10
        },
    }
    
    print("âš™ï¸  ä»»åŠ¡é…ç½®å¦‚ä¸‹:", flush=True)
    print(json.dumps({k:v for k,v in config.items() if k!='analysis_params'}, indent=2), flush=True)
    print("  \"analysis_params\":", json.dumps(config['analysis_params']), flush=True)
    
    # å¯åŠ¨æµæ°´çº¿
    pipeline = HierarchicalAnalysisPipeline(config)
    pipeline.run_batch_processing(video_files_to_process)


if __name__ == "__main__":
    main()
